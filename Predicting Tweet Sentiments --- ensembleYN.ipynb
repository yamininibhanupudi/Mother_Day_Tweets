{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Tweet Sentiments\n",
    "\n",
    "Goal of this competition: Building a model for Text Classification using Natural Language Processing. In other words, predicting which tweets are POSITIVE, NEGATIVE or NEUTRAL in the test set. This is a supervised learning problem, as the labels, \"sentiment_class\" were provided in the train.csv dataset. The performance of the model was evaluated using the following metric:\n",
    "100*f1_score(actual_values, predicted_values, average='weighted').\n",
    "\n",
    "The approach I had adopted was that of an ensemble model. I trained the Random Forests, Decision Trees, Extra Trees and Gradient Boosting Classifiers with the best possible parameters (that were fitted on the train set using GridSearchCV approach). Finally, used Max Voting technique to get the final_prediction that were made on the test set. Moreover, for the majority of the model I had used scikit-learn to develop it.\n",
    "\n",
    "Let's dive into code. We were provided with these two datasets, namely, train.csv and test.csv. Let's load them and take our first glance at the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as seab\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import the train and test csv files\n",
    "pd.set_option('display.max_colwidth', 100)   #to display 100 charactors in each column\n",
    "train_data = pd.read_csv(r'D:\\jobs\\A! Hackathon\\Mothers_Day\\dataset\\train.csv')\n",
    "test_data = pd.read_csv(r'D:\\jobs\\A! Hackathon\\Mothers_Day\\dataset\\test.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "The first step for creating an ML model is exploring the given datasets (train.csv and test.csv). This is a crucial step.\n",
    "\n",
    "To explore the data I had generated a report with pandas_profiling. It generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n",
    "\n",
    "For each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n",
    "\n",
    "- Type inference: detect the types of columns in a dataframe.\n",
    "- Essentials: type, unique values, missing values\n",
    "- Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n",
    "- Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n",
    "- Most frequent values\n",
    "- Histogram\n",
    "- Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n",
    "- Missing values matrix, count, heatmap and dendrogram of missing values\n",
    "- Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's generate a profile for test data as well\n",
    "pandas_profiling.ProfileReport(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the above profiles to make some inferences.\n",
    "\n",
    "The different variables in the dataset are:\n",
    "1. id: an index provided to every record in the datasets\n",
    "2. original_text: The actual Morther's Day tweets\n",
    "3. lang: The language of the tweets. In this dataset most of them are english, i.e., 'en'\n",
    "4. retweet_count: The number of times the texts were retweeted\n",
    "5. original_author: The person that posted the tweet (it is their username) \n",
    "6. sentiment_class: This is the 'target' variable. All the tweets were labelled as 1 (Positive), 0 (Neutral) or -1 (Negative). We are to create a Machine Learning model that uses Natural Language Processing to predict these labels on the test data.\n",
    "\n",
    "In the field 'sentiment_class', how many Positive (1), Negative(-1) and Neutral(0) texts are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "seab.countplot(x='sentiment_class', data=train_data)\n",
    "print('Out of {} rows, {} are labelled as 1, {} are labelled as 0 and {} are labelled as -1'.format(\n",
    "                                                                len(train_data), \n",
    "                                                                len(train_data[train_data['sentiment_class'] == 1]), \n",
    "                                                                len(train_data[train_data['sentiment_class'] == 0]),\n",
    "                                                                len(train_data[train_data['sentiment_class'] == -1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is a huge imbalance in the data labels. Shall we balance it using SMOTE? or Import a pretrained model?  But for using a deep learning model we need loads and loads of data. Therefore, to handle the imbalnce in data, I had used the trees classifiers which come with a class_weight variable that helps tackle this issue.\n",
    "\n",
    "Moreover, there seems to be only few missing values (<0.1%). So, I am going to let them be for now. Moreover, the variables 'lang', 'retweet_count' and 'original_author' have high cardinality. Groping them into similar categories or any other engineering would overfit the model to this dataset. Hence, we are going to drop these three variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the most important part of creating a Machine Learning model. More sensible the features considered are, the more accurate out model would be. In this context, that is, tweet classification, I believe the best feature would be that of a clean text.\n",
    "\n",
    "\"clean_original_text\" is the feature I created. Which is, the given tweets in field \"original_text\" were cleaned. The follwing cleaning was done:\n",
    "\n",
    "- Various factors such as URLs, hashtags, usernames, punctuations and stopwords were removed\n",
    "- Duplicated were removed, i.e., hellllooooooo was changed to hello \n",
    "- The tokenized words were stemmed. We do this by building a function called \"clean_text\". This function was called everytime we would want to clean the \"original_text\"\n",
    "\n",
    "The engineered feature was further vetorized using TF-IDF vectorizer.\n",
    "\n",
    "The above procedure was adopted under the assumption that cleansed and vectorized data could facilitate a more accurate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Generation ---- clean_original_text\n",
    "# Cleaning our data --- remove punctuations,stopwords, #, usernames and URLs also let's stem or lemmatize\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()    # the stemmer used here is PorterStemmer\n",
    "\n",
    "def clean_text(given_text):\n",
    "    text_noURL = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', given_text) # remove URLs\n",
    "    text_nouser = re.sub('@[^\\s]+', 'AT_USER', text_noURL)                       # remove usernames\n",
    "    text_nohash = re.sub(r'#([^\\s]+)', r'\\1', text_nouser)                        # remove the # in #hashtag\n",
    "    text_token = word_tokenize(text_nohash)           # remove repeated characters (helloooooooo into hello)\n",
    "    text_nopunct = \" \".join([word.lower() for word in text_token if word not in string.punctuation])    #lowercase and remove punctuations\n",
    "    text_tokenized = re.split('\\W+', text_nopunct)                                       # tokinize the text\n",
    "    text = [ps.stem(word) for word in text_tokenized if word not in stopwords]     # remove stop words and stem the non-stopwords\n",
    "    return text\n",
    "\n",
    "# Now let's vectorize using tfidf\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
    "X_features = tfidf_vect.fit_transform(train_data['original_text']).toarray()\n",
    "X_test = tfidf_vect.transform(test_data['original_text']).toarray()\n",
    "Y = train_data['sentiment_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection, Comparision and Evaluation\n",
    "\n",
    "The various classifiers trained were:\n",
    "1. Random Forest\n",
    "2. Decision Trees\n",
    "3. ExtraTrees Classifier\n",
    "4. Gradient Boosting Trees\n",
    "\n",
    "GridSearchCV was used to fit these models on best possible parameters. Further, these models were compared and finally an ensemble model was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "params = {'n_estimators' : [10, 15, 20], \n",
    "          'max_depth' : [205, 300, 305, 400]\n",
    "         }\n",
    "gs = GridSearchCV(rf, params, cv=5, scoring='f1_weighted', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt = DecisionTreeClassifier()\n",
    "params = {'max_depth' : [30, 50, 99], \n",
    "          'max_features' : [1000, 2000, 5000, 10000]\n",
    "         }\n",
    "gs = GridSearchCV(dt, params, cv=5, scoring='f1_weighted', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra trees\n",
    "etc = ExtraTreesClassifier(random_state=10)\n",
    "params = {'n_estimators' : [10, 50, 100, 500, 1000], \n",
    "          'max_features' : [5, 10, 50, 100]\n",
    "         }\n",
    "gs = GridSearchCV(etc, params, cv=5, scoring='f1_weighted', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Trees\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "params = {'n_estimators' : [10, 50, 100], \n",
    "          'max_depth' : [5, 10, 30, 60]\n",
    "         }\n",
    "gs = GridSearchCV(gb, params, cv=5, scoring='f1_weighted', verbose = 1, n_jobs = -1)\n",
    "gs.fit(X_features, Y)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model\n",
    "\n",
    "Further an ensemble model was created using Max Voting technique which was implemented using the VotingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, max_depth=300, class_weight='balanced', n_jobs=-1)\n",
    "dt = DecisionTreeClassifier(max_depth=99, max_features=400)\n",
    "etc = ExtraTreesClassifier(n_estimators=100, max_features=2000, n_jobs=-1) \n",
    "gb = GradientBoostingClassifier(n_estimators=10, max_depth=17, learning_rate=0.1)\n",
    "\n",
    "# Max Voting method\n",
    "ensemble = VotingClassifier(estimators=[('rf', rf), ('dt', dt), ('etc', etc), ('gb', gb)], voting='soft', n_jobs=-1, \n",
    "                            weights=[2,2,1,1], flatten_transform=True)\n",
    "ensemble.fit(X_features, Y)\n",
    "final_predictions = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the predictions to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'id': test_data.id, 'sentiment_class': final_predictions})\n",
    "output.to_csv(r'D:\\jobs\\A! Hackathon\\Mothers_Day\\dataset\\submission_rf5.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
